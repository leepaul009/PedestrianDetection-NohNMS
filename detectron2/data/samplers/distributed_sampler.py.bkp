# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved
import itertools
import math
from collections import defaultdict
from typing import Optional
import torch
from torch.utils.data.sampler import Sampler

from detectron2.utils import comm


class TrainingSampler(Sampler):
    """
    In training, we only care about the "infinite stream" of training data.
    So this sampler produces an infinite stream of indices and
    all workers cooperate to correctly shuffle the indices and sample different indices.

    The samplers in each worker effectively produces `indices[worker_id::num_workers]`
    where `indices` is an infinite stream of indices consisting of
    `shuffle(range(size)) + shuffle(range(size)) + ...` (if shuffle is True)
    or `range(size) + range(size) + ...` (if shuffle is False)
    """

    def __init__(self, size: int, shuffle: bool = True, seed: Optional[int] = None):
        """
        Args:
            size (int): the total number of data of the underlying dataset to sample from
            shuffle (bool): whether to shuffle the indices or not
            seed (int): the initial seed of the shuffle. Must be the same
                across all workers. If None, will use a random seed shared
                among workers (require synchronization among all workers).
        """
        self._size = size
        assert size > 0
        self._shuffle = shuffle
        if seed is None:
            seed = comm.shared_random_seed()
        self._seed = int(seed)

        self._rank = comm.get_rank()
        self._world_size = comm.get_world_size()

    def __iter__(self):
        start = self._rank
        yield from itertools.islice(self._infinite_indices(), start, None, self._world_size)

    def _infinite_indices(self):
        g = torch.Generator()
        g.manual_seed(self._seed)
        while True:
            if self._shuffle:
                yield from torch.randperm(self._size, generator=g)
            else:
                yield from torch.arange(self._size)


class RepeatFactorTrainingSampler(Sampler):
    """
    Similar to TrainingSampler, but suitable for training on class imbalanced datasets
    like LVIS. In each epoch, an image may appear multiple times based on its "repeat
    factor". The repeat factor for an image is a function of the frequency the rarest
    category labeled in that image. The "frequency of category c" in [0, 1] is defined
    as the fraction of images in the training set (without repeats) in which category c
    appears.

    See https://arxiv.org/abs/1908.03195 (>= v2) Appendix B.2.
    """

    def __init__(self, dataset_dicts, repeat_thresh, shuffle=True, seed=None):
        """
        Args:
            dataset_dicts (list[dict]): annotations in Detectron2 dataset format.
            repeat_thresh (float): frequency threshold below which data is repeated.
            shuffle (bool): whether to shuffle the indices or not
            seed (int): the initial seed of the shuffle. Must be the same
                across all workers. If None, will use a random seed shared
                among workers (require synchronization among all workers).
        """
        self._shuffle = shuffle
        if seed is None:
            seed = comm.shared_random_seed()
        self._seed = int(seed)

        self._rank = comm.get_rank()
        self._world_size = comm.get_world_size()

        # Get fractional repeat factors and split into whole number (_int_part)
        # and fractional (_frac_part) parts.
        rep_factors = self._get_repeat_factors(dataset_dicts, repeat_thresh)
        self._int_part = torch.trunc(rep_factors)
        self._frac_part = rep_factors - self._int_part

        self.additional_ids = None
        self.additional_losses = None
        self.device = None

    def _get_repeat_factors(self, dataset_dicts, repeat_thresh):
        """
        Compute (fractional) per-image repeat factors.

        Args:
            See __init__.

        Returns:
            torch.Tensor: the i-th element is the repeat factor for the dataset image
                at index i.
        """
        # 1. For each category c, compute the fraction of images that contain it: f(c)
        category_freq = defaultdict(int)
        for dataset_dict in dataset_dicts:  # For each image (without repeats)
            cat_ids = {ann["category_id"] for ann in dataset_dict["annotations"]}
            for cat_id in cat_ids:
                category_freq[cat_id] += 1
        num_images = len(dataset_dicts)
        for k, v in category_freq.items():
            category_freq[k] = v / num_images

        # 2. For each category c, compute the category-level repeat factor:
        #    r(c) = max(1, sqrt(t / f(c)))
        category_rep = {
            cat_id: max(1.0, math.sqrt(repeat_thresh / cat_freq))
            for cat_id, cat_freq in category_freq.items()
        }

        # 3. For each image I, compute the image-level repeat factor:
        #    r(I) = max_{c in I} r(c)
        rep_factors = []
        for dataset_dict in dataset_dicts:
            cat_ids = {ann["category_id"] for ann in dataset_dict["annotations"]}
            rep_factor = max({category_rep[cat_id] for cat_id in cat_ids})
            rep_factors.append(rep_factor)

        return torch.tensor(rep_factors, dtype=torch.float32)

    def _get_epoch_indices(self, generator):
        """
        Create a list of dataset indices (with repeats) to use for one epoch.

        Args:
            generator (torch.Generator): pseudo random number generator used for
                stochastic rounding.

        Returns:
            torch.Tensor: list of dataset indices to use in one epoch. Each index
                is repeated based on its calculated repeat factor.
        """
        # Since repeat factors are fractional, we use stochastic rounding so
        # that the target repeat factor is achieved in expectation over the
        # course of training
        rands = torch.rand(len(self._frac_part), generator=generator)
        rep_factors = self._int_part + (rands < self._frac_part).float()
        # Construct a list of indices in which we repeat images as specified
        indices = []
        for dataset_index, rep_factor in enumerate(rep_factors):
            indices.extend([dataset_index] * int(rep_factor.item()))

        # print(" * * * * * * R.{}  indices:{}".format( self._rank, len(indices) ))

        # train more on the data
        if torch.is_tensor(self.additional_ids) and torch.is_tensor(self.additional_losses):

            assert self.additional_ids.shape[0] == self.additional_losses.shape[0], "size not match!"

            
            # set self.device
            # synchronize
            # self.additional_ids    = self.additional_ids.to(self.device)
            # self.additional_losses = self.additional_losses.to(self.device)

            # size => size * world_size
            comm.synchronize()
            self.additional_ids    = comm.all_gather(self.additional_ids)
            self.additional_losses = comm.all_gather(self.additional_losses)
            comm.synchronize()

            # self.additional_ids    = [ it.cpu() for it in self.additional_ids ] 
            # self.additional_losses = [ it.cpu() for it in self.additional_losses ] 

            self.additional_losses = torch.cat(self.additional_losses)
            self.additional_ids    = torch.cat(self.additional_ids)

            # 3) move data to cpu
            # Question: does gpu buffer been relased after moving to cpu
            # self.additional_ids    = self.additional_ids.cpu()
            # self.additional_losses = self.additional_losses.cpu()
            


            sorted_loss, _sorted_key = torch.sort(self.additional_losses, descending=True)
            sorted_ids = self.additional_ids[_sorted_key]
            
            print(" + + + * * * * * * R.{}  indices:{}, sorted_ids: {}, sorted_loss: {}"
                .format( self._rank, len(indices), sorted_ids[:10], sorted_loss[:10] ))

            # sorted_ids = sorted_ids[:800]
            # for import_id in sorted_ids:
            #     rep_factor = 10 * 4
                # indices.extend([import_id] * int(rep_factor))
            
            items_per_thread = 200
            sorted_ids = sorted_ids[self._rank * items_per_thread : (self._rank + 1) * items_per_thread]
            rep_factor = 8
            for import_id in sorted_ids: 
                indices.extend([import_id] * int(rep_factor))
            
            
            # release 
            self.additional_ids = None
            self.additional_losses = None

        return torch.tensor(indices, dtype=torch.int64)

    def __iter__(self):
        start = self._rank
        yield from itertools.islice(self._infinite_indices(), start, None, self._world_size)

    def _infinite_indices(self):
        g = torch.Generator()
        g.manual_seed(self._seed)
        while True:
            # Sample indices with repeats determined by stochastic rounding; each
            # "epoch" may have a slightly different size due to the rounding.
            indices = self._get_epoch_indices(g)
            if self._shuffle:
                randperm = torch.randperm(len(indices), generator=g)
                yield from indices[randperm]
            else:
                yield from indices

    ### public func
    # cur_ids:          [batch_per_work,]
    # loss_per_image:   [batch_per_work,]
    def update_data_dicts(self, cur_ids, loss_per_image):
        if torch.is_tensor(self.additional_ids) and torch.is_tensor(self.additional_losses):
            self.additional_ids    = torch.cat([self.additional_ids,    cur_ids])
            self.additional_losses = torch.cat([self.additional_losses, loss_per_image])

            assert self.additional_ids.shape[0] == self.additional_losses.shape[0]
        else: # init
            self.additional_ids    = cur_ids
            self.additional_losses = loss_per_image

    def update_data_dicts_from_dict(self, cur_ids, loss_per_image, device):

        self.device = device

        if torch.is_tensor(self.additional_ids) and torch.is_tensor(self.additional_losses):
            
            self.additional_ids    = torch.cat([self.additional_ids, cur_ids])
            self.additional_losses = torch.cat([self.additional_losses, loss_per_image])

            assert self.additional_ids.shape[0] == self.additional_losses.shape[0]
            
            # print(" * * * * * * + + + R.{}  ids:{}, loss:{}"
            #     .format( self._rank, self.additional_ids, self.additional_losses ))
 
            # 4) control buffer size per thread
            threshold_to_drop = 400
            buffer_max_size = 210

            if self.additional_ids.shape[0] > threshold_to_drop:
                ### merge
                # 1) set self.device

                # 2) move data to device
                # self.additional_ids    = self.additional_ids.to(self.device)
                # self.additional_losses = self.additional_losses.to(self.device)

                # size => size * world_size
                comm.synchronize()
                self.additional_ids    = comm.all_gather(self.additional_ids)
                self.additional_losses = comm.all_gather(self.additional_losses)
                comm.synchronize()
                
                # self.additional_ids    = [ it for it in self.additional_ids ] 
                # self.additional_losses = [ it for it in self.additional_losses ] 

                self.additional_losses = torch.cat(self.additional_losses)
                self.additional_ids    = torch.cat(self.additional_ids)
                
                # self.additional_losses = list(itertools.chain(*self.additional_losses))
                # self.additional_ids = list(itertools.chain(*self.additional_ids))

                # 3) move data to cpu
                # Question: does gpu buffer been relased after moving to cpu
                # self.additional_ids    = self.additional_ids.cpu()
                # self.additional_losses = self.additional_losses.cpu()

                # sort
                _, _sorted_key = torch.sort(self.additional_losses, descending=True)
                # each thread take different buffer
                self.additional_ids = self.additional_ids[_sorted_key][self._rank * buffer_max_size : (self._rank+1) * buffer_max_size]
                self.additional_losses = self.additional_losses[_sorted_key][self._rank * buffer_max_size : (self._rank+1) * buffer_max_size]
            
                print(" * * * * * * after sync, R.{}  ids:{}, loss:{}"
                    .format( self._rank, self.additional_ids[:10], self.additional_losses[:10] ))

        else: # init
            self.additional_ids    = cur_ids
            self.additional_losses = loss_per_image


class InferenceSampler(Sampler):
    """
    Produce indices for inference.
    Inference needs to run on the __exact__ set of samples,
    therefore when the total number of samples is not divisible by the number of workers,
    this sampler produces different number of samples on different workers.
    """

    def __init__(self, size: int):
        """
        Args:
            size (int): the total number of data of the underlying dataset to sample from
        """
        self._size = size
        assert size > 0
        self._rank = comm.get_rank()
        self._world_size = comm.get_world_size()

        shard_size = (self._size - 1) // self._world_size + 1
        begin = shard_size * self._rank
        end = min(shard_size * (self._rank + 1), self._size)
        self._local_indices = range(begin, end)

    def __iter__(self):
        yield from self._local_indices

    def __len__(self):
        return len(self._local_indices)
